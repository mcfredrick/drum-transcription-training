# Storage Architecture

**Date:** February 7, 2026  
**Decision:** Keep project on SSD, use HDD for large data only

---

## Why This Architecture?

After extracting the 90GB E-GMD dataset and beginning project setup, we had to decide:
- Should the entire project live on the HDD with the dataset?
- Or keep the project on SSD and reference the dataset from HDD?

**We chose: SSD for project, HDD for data**

---

## Rationale

### Benefits of SSD for Project Code
1. **Fast development iteration** - Git operations, code editing, IDE indexing, and module imports are significantly faster on SSD
2. **Better workflow** - The project lives in the natural working directory (`~/Documents/`)
3. **Small footprint** - Project files (configs, Python scripts, notebooks) are tiny (~270KB) and won't grow significantly
4. **Python environment performance** - Package installs/updates and module imports are faster
5. **Development speed** - Code changes, linting, testing, and debugging benefit from SSD speed

### Why HDD is Acceptable for Data
1. **Sequential reads dominate** - Training loads audio files in batches sequentially, which HDDs handle reasonably well
2. **GPU is the bottleneck** - Once data is loaded, GPU computation time far exceeds I/O time
3. **Periodic writes** - Checkpoints and logs are saved occasionally, not continuously
4. **Size requirements** - Dataset (90GB) + processed data (~50-100GB) + checkpoints (~5-10GB) would consume valuable SSD space

---

## Directory Structure

### SSD: `/home/matt/Documents/drum-tranxn/drum_transcription/`
**Purpose:** Active development, code, configurations

```
drum_transcription/
â”œâ”€â”€ .venv/                  # Python virtual environment (UV managed)
â”œâ”€â”€ pyproject.toml          # Project dependencies
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ docs/                   # Documentation (this file)
â”‚   â””â”€â”€ storage-architecture.md
â”œâ”€â”€ configs/                # YAML configuration files
â”‚   â”œâ”€â”€ test_config.yaml    # Quick test with 20 files
â”‚   â””â”€â”€ full_config.yaml    # Full training configuration (to be created)
â”œâ”€â”€ notebooks/              # Jupyter notebooks for exploration
â”œâ”€â”€ scripts/                # Training and inference scripts
â”‚   â”œâ”€â”€ preprocess_egmd.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ transcribe.py
â””â”€â”€ src/                    # Source code modules
    â”œâ”€â”€ data/               # Data processing
    â”‚   â”œâ”€â”€ audio_processing.py
    â”‚   â”œâ”€â”€ midi_processing.py
    â”‚   â”œâ”€â”€ dataset.py
    â”‚   â””â”€â”€ data_module.py
    â”œâ”€â”€ models/             # Model implementations
    â”‚   â””â”€â”€ crnn.py
    â””â”€â”€ utils/              # Utility functions
        â””â”€â”€ metrics.py
```

**Estimated size:** < 10 MB (excluding .venv which is ~500MB-1GB but benefits from SSD)

### HDD: `/mnt/hdd/drum-tranxn/`
**Purpose:** Large datasets, processed data, training outputs

```
/mnt/hdd/drum-tranxn/
â”œâ”€â”€ e-gmd-v1.0.0/           # Original E-GMD dataset (90GB)
â”‚   â”œâ”€â”€ drummer1/
â”‚   â”œâ”€â”€ drummer2/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed_data/         # Preprocessed spectrograms & labels
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ checkpoints/            # Model checkpoints during training
â”‚   â”œâ”€â”€ test_run/
â”‚   â””â”€â”€ full_training/
â””â”€â”€ logs/                   # Training logs and tensorboard data
    â”œâ”€â”€ test_run/
    â””â”€â”€ full_training/
```

**Estimated size:** 150-250 GB total

---

## Configuration Pattern

All config files store HDD paths for data and outputs:

```yaml
data:
  egmd_root: "/mnt/hdd/drum-tranxn/e-gmd-v1.0.0"
  processed_dir: "/mnt/hdd/drum-tranxn/processed_data"

training:
  checkpoint_dir: "/mnt/hdd/drum-tranxn/checkpoints"
  log_dir: "/mnt/hdd/drum-tranxn/logs"
```

Code and configs are loaded from the SSD project directory.

---

## Performance Characteristics

### Fast Operations (SSD)
- âœ… Git operations (clone, commit, push, pull)
- âœ… IDE indexing and code completion
- âœ… Module imports (`import src.models.crnn`)
- âœ… Config file reads
- âœ… Code execution and testing
- âœ… Python package installs (`uv add`)

### Acceptable Operations (HDD)
- âš ï¸ Loading audio files during training (sequential, batched by DataLoader)
- âš ï¸ Reading preprocessed spectrograms (memory-mapped if needed)
- âš ï¸ Saving model checkpoints (periodic, not continuous)
- âš ï¸ Writing tensorboard logs (buffered)

### Bottleneck (GPU)
- ðŸš€ Forward/backward passes
- ðŸš€ Gradient computation
- ðŸš€ Spectrogram computation (can be GPU-accelerated)

**Result:** Training speed is GPU-bound, not I/O-bound

---

## Working with This Setup

### Starting a Development Session
```bash
cd ~/Documents/drum-tranxn/drum_transcription
uv run python scripts/train.py --config configs/test_config.yaml
```

### All data paths are resolved automatically via configs
- No need to `cd` to HDD locations
- Output paths in configs point to HDD
- Code imports work from SSD

### Version Control
```bash
cd ~/Documents/drum-tranxn/drum_transcription
git add .
git commit -m "Add data preprocessing module"
git push
```

**Note:** HDD data directories are NOT in version control (too large)

---

## Migration from Earlier Setup

An earlier attempt created a duplicate structure at `/mnt/hdd/drum-tranxn/drum_transcription/`.

**Action taken:**
- Kept primary project at `/home/matt/Documents/drum-tranxn/drum_transcription/` (SSD)
- Updated configs to reference HDD for data paths only
- HDD now only contains: dataset, processed_data/, checkpoints/, logs/

**Files to update:**
- `configs/test_config.yaml` - Already points to HDD paths
- New configs should follow the same pattern

---

## Future Considerations

### If SSD Space Becomes Tight
- The `.venv/` directory can be moved to HDD if needed (slower package operations)
- Use `WORKON_HOME` or symlinks to relocate virtual environment

### If Training is I/O Bound
- Preprocess all data to HDF5 or memory-mapped format on HDD
- Use `num_workers > 0` in DataLoader for parallel I/O
- Consider caching preprocessed data in RAM if training set fits (~16-32GB)

### If HDD Space Becomes Tight
- Move old checkpoints to archive storage
- Delete processed data and regenerate when needed (raw data is immutable)
- Use checkpoint compression or save fewer checkpoints

---

## Summary

**This architecture optimizes for:**
- âœ… Development speed and iteration
- âœ… Standard Python workflow
- âœ… Cost-effective storage for large datasets
- âœ… Git-friendly project structure

**Key principle:** Keep hot paths (code, imports, git) on SSD; cold paths (data storage, outputs) on HDD.
