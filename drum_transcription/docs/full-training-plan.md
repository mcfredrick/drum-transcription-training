# Full Training Plan - Drum Transcription

**Date:** February 7, 2026  
**Status:** Ready to Execute  
**Objective:** Train the drum transcription model on the full E-GMD dataset (4,148 files) for production-quality results.

---

## Executive Summary

### Medium Test Results ‚úÖ

The medium-duration test (500 files, 10 epochs) completed successfully and validated the approach:

| Metric | Result | Status |
|--------|--------|--------|
| **Training Completion** | 10/10 epochs completed | ‚úÖ Success |
| **Training Time** | ~5 minutes | ‚úÖ Much faster than estimated |
| **Initial Validation Loss** | 0.214 (epoch 0) | ‚úÖ Baseline established |
| **Final Validation Loss** | 0.124 (epoch 9) | ‚úÖ **42% improvement** |
| **Loss Trend** | Consistently decreasing | ‚úÖ Model learning well |
| **Best Checkpoint** | `medium-test-epoch=09-val_loss=0.1240.ckpt` | ‚úÖ Saved |
| **Files Processed** | 457/500 (91.4%) | ‚úÖ Good success rate |
| **Failed Files** | 43 (missing MIDI files) | ‚úÖ Automatically filtered |

### Key Achievements

1. ‚úÖ **MIDI File Extension Issues Resolved** - System now handles both `.mid` and `.midi` extensions, automatically filters out files with missing MIDI files
2. ‚úÖ **Preprocessing Cache Working** - Already processed files are skipped, enabling incremental preprocessing
3. ‚úÖ **Training Pipeline Stable** - No errors, automatic checkpointing working
4. ‚úÖ **Consistent Learning** - Validation loss decreased steadily across all 10 epochs
5. ‚úÖ **Fast Training Speed** - 457 files trained in ~5 minutes (~40 files/minute)

### Decision: Proceed to Full Training

Based on the successful medium test, we are ready to scale up to the full dataset.

---

## Full Training Configuration

### Dataset Specifications

| Parameter | Value | Notes |
|-----------|-------|-------|
| **Total Audio Files** | 4,148 | Full E-GMD dataset |
| **Successfully Preprocessed** | ~3,783 (est. 91.4%) | Based on medium test success rate |
| **Train Split** | ~2,648 files (70%) | |
| **Validation Split** | ~567 files (15%) | |
| **Test Split** | ~567 files (15%) | |

### Training Parameters

| Parameter | Value | Reasoning |
|-----------|-------|-----------|
| **Epochs** | 100 | Standard for deep learning on music |
| **Batch Size** | 8 | Optimal for RTX 3070 (8GB VRAM) |
| **Learning Rate** | 0.001 | Adam default, proven stable |
| **Precision** | 16-mixed | 2x faster, minimal accuracy loss |
| **Early Stopping** | Enabled (patience: 25, min_delta: 0.0005) | Optimized for full training, more patient convergence |
| **Gradient Clipping** | 1.0 | Prevent exploding gradients |
| **Augmentation** | Disabled | Focus on learning core patterns first |

### Hardware Configuration

| Resource | Specification | Usage |
|----------|--------------|-------|
| **GPU** | NVIDIA RTX 3070 (8GB) | Training acceleration |
| **CPU Workers** | 8 | Parallel data loading |
| **Storage** | 2TB HDD `/mnt/hdd` | Preprocessed data + checkpoints |
| **RAM** | System default | Data loading buffer |

---

## Time & Resource Estimates

### Preprocessing Phase

Based on medium test performance (500 files in ~20 minutes):

| Stage | Files | Estimated Time | Notes |
|-------|-------|----------------|-------|
| **New Files** | ~3,648 | ~2.5 hours | Files not yet preprocessed |
| **Cached Files** | ~457 | Instant | Already preprocessed from medium test |
| **Total** | ~4,148 | **~2.5 hours** | With 8 workers |

### Training Phase

Based on medium test performance (457 files, 10 epochs in ~5 minutes):

| Scenario | Epochs | Estimated Time | Notes |
|----------|--------|----------------|-------|
| **Best Case** | 30-40 | **8-12 hours** | Early stopping triggers |
| **Typical Case** | 50-60 | **16-20 hours** | Steady improvement |
| **Worst Case** | 100 | **32-36 hours** | Full training, no early stopping |
| **Most Likely** | **50** | **~16 hours** | Based on typical convergence |

### Total Project Time

| Phase | Time | Notes |
|-------|------|-------|
| Preprocessing | ~2.5 hours | One-time cost (cached thereafter) |
| Training | ~16 hours | Most likely scenario (50 epochs) |
| Evaluation | ~30 minutes | Test set evaluation + metrics |
| **TOTAL** | **~19 hours** | **Can run overnight** |

---

## Storage Requirements

### Full Dataset Storage

| Component | Size | Location | Notes |
|-----------|------|----------|-------|
| **Raw E-GMD Data** | ~8 GB | `/mnt/hdd/drum-tranxn/e-gmd-v1.0.0` | Already downloaded |
| **Preprocessed Data (4,148 files)** | ~18-22 GB | `/mnt/hdd/drum-tranxn/processed_data` | HDF5 format |
| **Checkpoints (top 3)** | ~75 MB | `/mnt/hdd/drum-tranxn/checkpoints` | Best models only |
| **TensorBoard Logs** | ~200-500 MB | `/mnt/hdd/drum-tranxn/logs` | Training metrics |
| **Total (New)** | **~20-25 GB** | **HDD** | Plenty of space available |

Current HDD usage: ~8 GB (raw data only)  
Available space: ~1.8 TB  
**‚úÖ Plenty of headroom**

---

## Implementation Steps

### Step 1: Create Full Training Configuration

**File:** `configs/full_training_config.yaml`

**Action:** Create config file with production settings.

**Key Parameters:**
```yaml
data:
  max_files: null  # Use all files (don't limit)
  egmd_root: "/mnt/hdd/drum-tranxn/e-gmd-v1.0.0"
  processed_root: "/mnt/hdd/drum-tranxn/processed_data"
  splits_dir: "/mnt/hdd/drum-tranxn/processed_data/splits"

training:
  num_epochs: 100
  batch_size: 8
  learning_rate: 0.001
  precision: "16-mixed"

early_stopping:
  enabled: true
  patience: 25
  monitor: "val_loss"
  mode: "min"
  min_delta: 0.0005

checkpoint:
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  save_last: true
  dirpath: "/mnt/hdd/drum-tranxn/checkpoints"
  filename: "full-training-{epoch:02d}-{val_loss:.4f}"

logging:
  experiment_name: "full-training-4148files"
  save_dir: "/mnt/hdd/drum-tranxn/logs"

hardware:
  devices: [0]
  num_workers: 8
  accelerator: "gpu"
```

**Command:**
```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription
cp configs/medium_test_config.yaml configs/full_training_config.yaml
# Then edit the file to update parameters as shown above
```

---

### Step 2: Preprocess Full Dataset

**Command:**
```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

# Run preprocessing (cached files will be skipped)
uv run python scripts/preprocess_egmd.py \
  --config configs/full_training_config.yaml \
  --num-workers 8
```

**Expected Output:**
```
Creating dataset splits...
Found 4148 audio files
Split sizes: Train=2903, Val=622, Test=623
Splits saved to /mnt/hdd/drum-tranxn/processed_data/splits

Preprocessing 4148 files...
Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4148/4148 [2:30:00<00:00, 0.46it/s]

Preprocessing complete!
Success: 3783/4148
Failed: 365/4148

Some files failed to process. Check warnings above.
Updating split files to exclude failed files...
Updated splits: Train=2648, Val=567, Test=568
Removed 365 failed files from splits.
```

**Verification:**
```bash
# Count processed files
find /mnt/hdd/drum-tranxn/processed_data -name "*.h5" | wc -l

# Check split file sizes
wc -l /mnt/hdd/drum-tranxn/processed_data/splits/*.txt

# Expected output:
#  2648 train_split.txt
#   567 val_split.txt
#   568 test_split.txt
```

---

### Step 3: Launch Full Training (Robust Script)

**Recommended:** Use the robust training script which handles interruptions and retries:

```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

# Run in background with nohup (survives SSH disconnection)
nohup ./scripts/train_robust.sh configs/full_training_config.yaml > /tmp/full_training.log 2>&1 &

# Get process ID
echo $!  # Save this number in case you need to check/kill the process
```

**Alternative:** Direct training (no retry logic):

```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

nohup uv run python scripts/train.py \
  --config configs/full_training_config.yaml \
  > /tmp/full_training.log 2>&1 &
```

---

### Step 4: Monitor Training Progress

#### Real-Time Monitoring

**Option 1: Watch Log File**
```bash
# Follow training output
tail -f /tmp/full_training.log

# Watch for epoch completions
tail -f /tmp/full_training.log | grep "Epoch"
```

**Option 2: TensorBoard (Recommended)**
```bash
# Launch TensorBoard (in separate terminal or screen session)
tensorboard --logdir /mnt/hdd/drum-tranxn/logs

# Open in browser: http://localhost:6006
# View real-time graphs of:
#   - Training loss
#   - Validation loss
#   - Learning rate
#   - Epoch time
```

**Option 3: Check Status Commands**
```bash
# Is training still running?
ps aux | grep train.py

# GPU usage
nvidia-smi

# Watch GPU usage (refreshes every 1 second)
watch -n 1 nvidia-smi

# Latest checkpoint created
ls -lth /mnt/hdd/drum-tranxn/checkpoints/ | head -5

# Current epoch (from logs)
grep -E "Epoch [0-9]+," /mnt/hdd/drum-tranxn/logs/*.log | tail -5
```

#### Progress Indicators

**Healthy Training Signs:**
- ‚úÖ Validation loss decreasing over epochs
- ‚úÖ Training loss decreasing smoothly
- ‚úÖ GPU utilization 80-100%
- ‚úÖ New checkpoints being saved periodically
- ‚úÖ No error messages in logs

**Warning Signs:**
- ‚ö†Ô∏è Validation loss increasing (overfitting)
- ‚ö†Ô∏è Training loss not decreasing (learning rate too high/low)
- ‚ö†Ô∏è GPU utilization <50% (data loading bottleneck)
- ‚ö†Ô∏è Training stopped unexpectedly (check logs)

---

### Step 5: Handle Interruptions (If Needed)

Training is designed to be robust to interruptions. If training stops:

**Check What Happened:**
```bash
# Check if process is still running
ps aux | grep train.py

# Check last lines of log
tail -50 /tmp/full_training.log

# Check latest checkpoint
ls -lth /mnt/hdd/drum-tranxn/checkpoints/ | head -3
```

**Resume Training:**
```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

# Resume from last checkpoint
nohup ./scripts/train_robust.sh configs/full_training_config.yaml \
  > /tmp/full_training_resume.log 2>&1 &
```

The training script automatically detects and resumes from the latest checkpoint.

---

### Step 6: Evaluate Best Model

Once training completes (or early stopping triggers):

**Find Best Checkpoint:**
```bash
# List checkpoints sorted by validation loss (best first)
ls -1v /mnt/hdd/drum-tranxn/checkpoints/full-training-*.ckpt | grep -v last | head -3
```

**Run Evaluation:**
```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

# Set best checkpoint path
BEST_CKPT=$(ls -1v /mnt/hdd/drum-tranxn/checkpoints/full-training-*.ckpt | grep -v last | head -1)

# Evaluate on test set
uv run python scripts/evaluate.py \
  --checkpoint "$BEST_CKPT" \
  --config configs/full_training_config.yaml \
  --split test \
  --output docs/full-training-eval-results.json
```

**Expected Output:**
```
Loading checkpoint: /mnt/hdd/drum-tranxn/checkpoints/full-training-epoch=45-val_loss=0.0892.ckpt
Loading test set...
Evaluating on 568 files...

Evaluation Results:
===================

Per-Class Metrics:
                precision    recall  f1-score   support

        kick       0.85      0.88      0.86      4523
       snare       0.82      0.84      0.83      3891
       hihat       0.78      0.81      0.79      5234
      hi_tom       0.71      0.68      0.69       892
     mid_tom       0.69      0.65      0.67       743
     low_tom       0.72      0.70      0.71      1023
       crash       0.76      0.73      0.74      1234
        ride       0.74      0.77      0.75      1567

    accuracy                           0.79     19107
   macro avg       0.76      0.76      0.76     19107
weighted avg       0.79      0.79      0.79     19107

Overall F1-Score: 0.79
Test Loss: 0.0892
```

---

### Step 7: Test Transcription on Real Audio

**Transcribe a Test File:**
```bash
cd /home/matt/Documents/drum-tranxn/drum_transcription

# Pick a test audio file
TEST_AUDIO=$(head -1 /mnt/hdd/drum-tranxn/processed_data/splits/test_split.txt | sed 's/\.wav//')
TEST_AUDIO="/mnt/hdd/drum-tranxn/e-gmd-v1.0.0/${TEST_AUDIO}.wav"

# Get best checkpoint
BEST_CKPT=$(ls -1v /mnt/hdd/drum-tranxn/checkpoints/full-training-*.ckpt | grep -v last | head -1)

# Transcribe
uv run python scripts/transcribe.py \
  "$TEST_AUDIO" \
  /tmp/test_transcription.mid \
  --checkpoint "$BEST_CKPT"
```

**Test on Your Own Audio:**
```bash
# Transcribe any drum audio file
uv run python scripts/transcribe.py \
  /path/to/your/drums.wav \
  /tmp/my_transcription.mid \
  --checkpoint "$BEST_CKPT"

# Open the MIDI file in your DAW or:
timidity /tmp/my_transcription.mid  # Play MIDI (if timidity installed)
```

---

## Success Criteria

### Minimum Acceptable Performance

After full training on 3,783 files, we expect:

| Metric | Target | Notes |
|--------|--------|-------|
| **Overall F1-Score** | >0.60 | Usable for real applications |
| **Validation Loss** | <0.12 | Better than medium test (0.124) |
| **Kick F1** | >0.75 | Most important drum |
| **Snare F1** | >0.70 | Second most important |
| **Hi-hat F1** | >0.65 | High frequency, harder to detect |
| **Training Stability** | No crashes | Completes without errors |

### Production-Ready Performance

| Metric | Target | Notes |
|--------|--------|-------|
| **Overall F1-Score** | >0.75 | Professional quality |
| **Validation Loss** | <0.10 | Strong generalization |
| **Common Drums (kick/snare/hihat)** | F1 >0.80 | Excellent accuracy |
| **All Drums** | F1 >0.65 | Good across all classes |

---

## Troubleshooting

### Training Crashes

**Symptom:** Process dies, logs show error  
**Common Causes:**
1. **Out of Memory (OOM)**
   - Reduce batch size in config: `batch_size: 4` or `batch_size: 2`
   - Restart training (will resume from checkpoint)

2. **CUDA Out of Memory**
   - Clear GPU cache: `nvidia-smi | grep python | awk '{print $5}' | xargs -I {} kill {}`
   - Reduce batch size or disable mixed precision

3. **Data Loading Error**
   - Check failed file logs in preprocessing output
   - Verify split files don't contain non-existent files

### Training Too Slow

**Symptom:** <10 batches/second  
**Solutions:**
1. Increase `num_workers` in config (try 12 or 16)
2. Check GPU utilization with `nvidia-smi` (should be 90%+)
3. Ensure data is on HDD, not network drive
4. Enable mixed precision: `precision: "16-mixed"`

### Validation Loss Not Decreasing

**Symptom:** Loss plateaus after few epochs  
**Solutions:**
1. Let it run longer (sometimes improves after 20-30 epochs)
2. Try lower learning rate: `learning_rate: 0.0005`
3. Enable augmentation for more training variation
4. Check if training loss is also stuck (might be architecture issue)

### Early Stopping Too Aggressive

**Symptom:** Training stops early, but you think it could improve  
**Solutions:**
1. Increase patience: `patience: 35` or `patience: 40`
2. Reduce min_delta: `min_delta: 0.0003` for even less restrictive stopping
3. Disable early stopping: `enabled: false`
4. Examine TensorBoard - if loss is truly flat, early stopping was correct

---

## Next Steps After Training

### If Results Are Good (F1 >0.60)

1. **Document Results**
   - Save evaluation metrics to docs folder
   - Create example transcriptions with comparisons
   - Document model performance characteristics

2. **Deploy Model**
   - Create inference API/script for easy use
   - Test on diverse audio sources (not just E-GMD)
   - Share checkpoints if open-sourcing

3. **Iterate**
   - Try training with augmentation enabled
   - Experiment with larger model (more layers/units)
   - Fine-tune on specific musical styles

### If Results Are Mediocre (F1 0.40-0.60)

1. **Analyze Errors**
   - Which drum classes perform worst?
   - Look at specific failure cases
   - Check if false positives or false negatives dominate

2. **Try Improvements**
   - Enable data augmentation
   - Adjust class weights for imbalanced drums
   - Increase model capacity
   - Train for more epochs (150-200)

3. **Architectural Changes**
   - Add attention mechanism
   - Try different RNN types (LSTM vs GRU)
   - Experiment with deeper CNNs

### If Results Are Poor (F1 <0.40)

1. **Debug Fundamentals**
   - Verify preprocessing is correct (check spectrograms)
   - Verify MIDI alignment is correct
   - Check if model is actually training (loss decreasing?)
   - Validate data loader is working (inspect batches)

2. **Simplify Problem**
   - Train on just kick/snare/hihat (3 classes)
   - Use larger dataset split for training (85/10/5)
   - Disable regularization (dropout)

3. **Seek Alternative Approaches**
   - Try simpler baseline model
   - Research state-of-the-art drum transcription papers
   - Consider pre-trained audio models (transfer learning)

---

## Risk Mitigation

### Hardware Risks

| Risk | Mitigation | Recovery |
|------|------------|----------|
| GPU overheating | Monitor with `nvidia-smi`, ensure good cooling | Training will resume from checkpoint |
| Power outage | Use UPS if available | Robust script auto-resumes |
| SSH disconnection | Use `nohup` or `screen` | Training continues in background |

### Software Risks

| Risk | Mitigation | Recovery |
|------|------------|----------|
| Out of memory | Batch size 8 tested on RTX 3070 | Reduce batch size, restart |
| Data corruption | HDF5 files validated during preprocessing | Re-run preprocessing with `--force` |
| Checkpoint corruption | Save top-3, not just top-1 | Use second-best checkpoint |

### Time Risks

| Risk | Mitigation | Recovery |
|------|------------|----------|
| Training takes >36 hours | Early stopping enabled (patience: 25) | Let it run, check if improving |
| Need to stop early | Automatic checkpointing every epoch | Resume later from checkpoint |
| Preprocessing too slow | 8 workers, cached files | Can't speed up much, just wait |

---

## Timeline & Milestones

### Day 1 (Today - Feb 7, 2026)

- **Hour 0-3:** Preprocessing full dataset (~2.5 hours)
- **Hour 3-19:** Training running (~16 hours estimated)
- **Before bed:** Verify training started successfully, check first few epochs

### Day 2 (Feb 8, 2026)

- **Morning:** Check training progress, should be 30-50% done
- **Afternoon:** Training likely completes if early stopping triggers
- **Evening:** Evaluate best model, analyze results

### Day 3 (Feb 9, 2026)

- **Morning:** If training still running, let complete
- **Afternoon:** Final evaluation, create transcription examples
- **Evening:** Document results, decide next steps

---

## Configuration File Template

**File:** `configs/full_training_config.yaml`

```yaml
# Full training configuration for production drum transcription model
# Training on complete E-GMD dataset (4,148 files)

data:
  # E-GMD dataset paths (on HDD)
  egmd_root: "/mnt/hdd/drum-tranxn/e-gmd-v1.0.0"
  processed_root: "/mnt/hdd/drum-tranxn/processed_data"
  splits_dir: "/mnt/hdd/drum-tranxn/processed_data/splits"
  
  # Use all files (no limit)
  max_files: null
  
  # Train/val/test split ratios
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  random_seed: 42

# Audio preprocessing parameters
audio:
  sample_rate: 22050
  n_fft: 2048
  hop_length: 512
  n_mels: 128
  fmin: 30
  fmax: 11025

# Model architecture
model:
  name: "DrumTranscriptionCRNN"
  n_mels: 128
  n_classes: 8  # kick, snare, hihat, hi_tom, mid_tom, low_tom, crash, ride
  
  # CNN parameters
  conv_filters: [32, 64, 128]
  conv_kernel_size: 3
  pool_size: 2
  dropout_cnn: 0.25
  
  # RNN parameters
  hidden_size: 128
  num_gru_layers: 2
  dropout_gru: 0.3
  bidirectional: true

# Training hyperparameters
training:
  num_epochs: 100  # Will likely stop earlier due to early stopping
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "Adam"
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 5
  scheduler_factor: 0.5
  
  # Optimization
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: "16-mixed"  # 2x faster on RTX 3070

# Data augmentation
augmentation:
  enabled: false  # Start without augmentation, enable later if needed
  time_stretch_prob: 0.0
  pitch_shift_prob: 0.0
  volume_scale_prob: 0.0
  reverb_prob: 0.0
  noise_prob: 0.0

# Loss function
loss:
  type: "BCELoss"
  class_weights: null  # Equal weights for all classes

# Early stopping (enabled for production)
early_stopping:
  enabled: true
  monitor: "val_loss"
  patience: 25  # Optimized for full training
  mode: "min"
  min_delta: 0.0005  # Less restrictive threshold

# Checkpointing
checkpoint:
  monitor: "val_loss"
  save_top_k: 3  # Keep top 3 best checkpoints
  mode: "min"
  save_last: true  # Always save last checkpoint for resuming
  dirpath: "/mnt/hdd/drum-tranxn/checkpoints"
  filename: "full-training-{epoch:02d}-{val_loss:.4f}"

# Logging
logging:
  logger: "tensorboard"
  project_name: "drum-transcription-production"
  experiment_name: "full-training-4148files"
  log_every_n_steps: 50
  save_dir: "/mnt/hdd/drum-tranxn/logs"

# Hardware configuration
hardware:
  devices: [0]  # RTX 3070
  num_workers: 8
  accelerator: "gpu"

# Postprocessing parameters
postprocessing:
  onset_threshold: 0.5
  min_onset_interval: 0.05  # 50ms minimum between onsets
  peak_min_distance: 2

# MIDI export configuration
midi:
  drum_names: ["kick", "snare", "hihat", "hi_tom", "mid_tom", "low_tom", "crash", "ride"]
  gm_mapping:
    kick: 36
    snare: 38
    hihat: 42
    hi_tom: 50
    mid_tom: 47
    low_tom: 45
    crash: 49
    ride: 51
  default_velocity: 80
  note_duration_ticks: 50

# E-GMD MIDI note mapping
egmd_midi_mapping:
  36: 0  # Kick
  38: 1  # Snare
  42: 2  # Hi-hat (closed)
  44: 2  # Pedal Hi-Hat -> Hi-Hat
  46: 2  # Open Hi-Hat -> Hi-Hat
  50: 3  # High Tom
  47: 4  # Mid Tom
  48: 4  # Hi-Mid Tom -> Mid Tom
  45: 5  # Low Tom
  41: 5  # Floor Tom -> Low Tom
  43: 5  # Floor Tom (high) -> Low Tom
  49: 6  # Crash
  55: 6  # Splash Cymbal -> Crash
  57: 6  # Crash Cymbal 2 -> Crash
  51: 7  # Ride
  53: 7  # Ride Bell -> Ride
  59: 7  # Ride Cymbal 2 -> Ride
```

---

## Quick Start Commands

**Copy-paste these commands to start full training:**

```bash
# 1. Create configuration (if not already done)
cd /home/matt/Documents/drum-tranxn/drum_transcription
cp configs/medium_test_config.yaml configs/full_training_config.yaml
# Edit full_training_config.yaml: set max_files: null, num_epochs: 100

# 2. Preprocess full dataset
uv run python scripts/preprocess_egmd.py \
  --config configs/full_training_config.yaml \
  --num-workers 8

# 3. Start training (robust, handles interruptions)
nohup ./scripts/train_robust.sh configs/full_training_config.yaml \
  > /tmp/full_training.log 2>&1 &

# 4. Save process ID for monitoring
echo $! > /tmp/training_pid.txt

# 5. Monitor progress (in another terminal)
tail -f /tmp/full_training.log

# OR: Use TensorBoard
tensorboard --logdir /mnt/hdd/drum-tranxn/logs
# Open browser: http://localhost:6006
```

---

## Summary

This plan provides a clear, executable path to train a production-quality drum transcription model on the full E-GMD dataset. The medium test validated the approach, and we're now ready to scale up.

**Key Points:**
- ‚úÖ All systems validated and working
- ‚úÖ Estimated time: ~19 hours total (preprocessing + training)
- ‚úÖ Can run overnight or over weekend
- ‚úÖ Robust to interruptions
- ‚úÖ Automatic checkpointing and resume
- ‚úÖ Clear success criteria and troubleshooting

**Ready to execute!** ü•ÅüöÄ
